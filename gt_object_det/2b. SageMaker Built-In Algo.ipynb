{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boots 'n' Cats 2b: Modelling with SageMaker Built-In Algorithm\n",
    "\n",
    "In this notebook we'll try another approach to build our boots 'n' cats detector: the [SageMaker built-in Object Detection algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html).\n",
    "\n",
    "Like Rekognition, this method doesn't need us to implement an algorithm, but this time we'll need to *know* more about deep neural networks to tune the model for good performance.\n",
    "\n",
    "**You'll need to** have gone through the first notebook in this series (*Intro and Data Preparation*) to complete this example.\n",
    "\n",
    "## About the Algorithm: SSD\n",
    "\n",
    "Like most of the built-in algorithms, the Object Detection docs include a [How It Works](https://docs.aws.amazon.com/sagemaker/latest/dg/algo-object-detection-tech-notes.html) with an overview and links to relevant resources.\n",
    "\n",
    "SageMaker Object Detection uses a Single Shot multi-box Detector algorithm as described in [Liu et al, 2016](https://arxiv.org/pdf/1512.02325.pdf).\n",
    "\n",
    "The object detection / bounding box problem is by no means easy, and our toy data-set is both small and diverse: So we don't anticipate amazing performance in this example, and should expect the built-in model pre-training to be very influential on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Dependencies and configuration\n",
    "\n",
    "As usual we'll start by loading libraries, defining configuration, and connecting to the AWS SDKs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# Built-Ins:\n",
    "import csv\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import imageio\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Local Dependencies:\n",
    "%aimport util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we re-load configuration from the intro & data processing notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r BUCKET_NAME\n",
    "assert BUCKET_NAME, \"BUCKET_NAME missing from IPython store\"\n",
    "%store -r CHECKPOINTS_PREFIX\n",
    "assert CHECKPOINTS_PREFIX, \"CHECKPOINTS_PREFIX missing from IPython store\"\n",
    "%store -r DATA_PREFIX\n",
    "assert DATA_PREFIX, \"DATA_PREFIX missing from IPython store\"\n",
    "%store -r MODELS_PREFIX\n",
    "assert MODELS_PREFIX, \"MODELS_PREFIX missing from IPython store\"\n",
    "%store -r CLASS_NAMES\n",
    "assert CLASS_NAMES, \"CLASS_NAMES missing from IPython store\"\n",
    "%store -r test_image_folder\n",
    "assert test_image_folder, \"test_image_folder missing from IPython store\"\n",
    "\n",
    "%store -r attribute_names\n",
    "assert attribute_names, \"attribute_names missing from IPython store\"\n",
    "%store -r n_samples_training\n",
    "assert n_samples_training, \"n_samples_training missing from IPython store\"\n",
    "%store -r n_samples_validation\n",
    "assert n_samples_validation, \"n_samples_validation missing from IPython store\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just connect to the AWS SDKs we'll use, and validate the choice of S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "s3 = session.resource(\"s3\")\n",
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "smclient = session.client(\"sagemaker\")\n",
    "\n",
    "bucket_region = \\\n",
    "    session.client(\"s3\").head_bucket(Bucket=BUCKET_NAME)[\"ResponseMetadata\"][\"HTTPHeaders\"][\"x-amz-bucket-region\"]\n",
    "assert (\n",
    "    bucket_region == region\n",
    "), f\"Your S3 bucket {BUCKET_NAME} and this notebook need to be in the same region.\"\n",
    "\n",
    "if (region != \"us-east-1\"):\n",
    "    print(\"WARNING: Rekognition Custom Labels functionality is only available in us-east-1 at launch\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Review our algorithm details\n",
    "\n",
    "The first step in deciding to use a SageMaker built-in algorithm is to review its [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html) and [common parameters](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html): To understand it's input/output interface, tunable parameters, use case, etc.\n",
    "\n",
    "In particular we'll need the URL for the Docker image in order to use the algorithm. While this is listed [in the docs](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html), it's also nice and easy to fetch programmatically.\n",
    "\n",
    "(Note some built-in algorithms have native classes in the SageMaker SDK e.g. `sagemaker.KMeans`: We only need this `training_image` URL for custom algorithms or built-ins like this one which the SDK treats as generic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = sagemaker.amazon.amazon_estimator.get_image_uri(\n",
    "    region,\n",
    "    \"object-detection\",\n",
    "    repo_version=\"latest\"\n",
    ")\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set up input data channels\n",
    "\n",
    "SageMaker describes data connections in terms of **channels**, rather than \"folders\" or \"sources\", to try and avoid any inaccurate assumptions about how algorithms see the connection and what API is presented.\n",
    "\n",
    "In this case we have in S3 for each of training and validation:\n",
    "\n",
    "* A *JSONLines manifest file* listing what images are in the data-set (by their S3 URI) and what annotations have been collected for those images (bounding boxes from SageMaker Ground Truth)\n",
    "* The image files themselves\n",
    "\n",
    "We'd like SageMaker to provide the algorithm with a **stream of image records** comprising both the image data and the annotations: To avoid having to wait around downloading the full dataset to the container before training starts; or retrieving the image bytes for each annotation.\n",
    "\n",
    "The [algorithm docs](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html#object-detection-inputoutput) give guidance on how to set this up: SageMaker already provides functionality to create RecordIO files for us from manifests. Note that (since SageMaker is handling construction of the stream and our algorithm isn't buffering everything in in one go), it's SageMaker that'll need to [shuffle](https://docs.aws.amazon.com/sagemaker/latest/dg/API_ShuffleConfig.html) the training data for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = sagemaker.session.s3_input(\n",
    "    f\"s3://{BUCKET_NAME}/{DATA_PREFIX}/train.manifest\",\n",
    "    distribution=\"FullyReplicated\",  # In case we want to try distributed training\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"AugmentedManifestFile\",\n",
    "    record_wrapping=\"RecordIO\",\n",
    "    attribute_names=attribute_names,  # In case the manifest contains other junk to ignore (it does!)\n",
    "    shuffle_config=sagemaker.session.ShuffleConfig(seed=1337),\n",
    ")\n",
    "                                        \n",
    "validation_channel = sagemaker.session.s3_input(\n",
    "    f\"s3://{BUCKET_NAME}/{DATA_PREFIX}/validation.manifest\",\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    record_wrapping=\"RecordIO\",\n",
    "    s3_data_type=\"AugmentedManifestFile\",\n",
    "    attribute_names=attribute_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure the algorithm\n",
    "\n",
    "The remainder of the pre-training setup concerns:\n",
    "\n",
    "* Output data connection parameters (where to store final model artifacts and intermediate checkpoints)\n",
    "* Compute resource specification\n",
    "* Algorithm (hyper-) parameters\n",
    "\n",
    "We do this through the SageMaker SDK's `Estimator` API, similarly to estimators in other common frameworks.\n",
    "\n",
    "Note:\n",
    "\n",
    "* \"Pipe mode\" streams input data to the algorithm rather than (the default) downloading the data up-front. This can accelerate training start-up for algorithms that support it.\n",
    "* As detailed in the [common parameters](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html) docs, `object-detection` supports GPU-accelerated and distributed training. We use a GPU-accelerated `ml.p3.2xlarge` instance type but don't bother to create more than one instance type due to the small data-set size.\n",
    "* Always prefer [spot instance](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html) training where practical: It's an easy way to save ~70-90% on training costs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    training_image,  # URL to container image implementing the algorithm \n",
    "    role,  # IAM access to perform the API actions\n",
    "    input_mode=\"Pipe\",\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.p3.2xlarge\",\n",
    "    train_volume_size=50,  # Make sure we don't run out of space\n",
    "    train_max_run = 5*60*60,\n",
    "    train_use_spot_instances=True,\n",
    "    train_max_wait= 5*60*60,\n",
    "    base_job_name=\"bootsncats-ssd\",\n",
    "    output_path=f\"s3://{BUCKET_NAME}/{MODELS_PREFIX}\",\n",
    "    checkpoint_s3_uri=f\"s3://{BUCKET_NAME}/{CHECKPOINTS_PREFIX}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(\n",
    "    # Pre-training is particularly important for tiny data-sets like this!:\n",
    "    base_network=\"resnet-50\",\n",
    "    early_stopping=True,\n",
    "    early_stopping_min_epochs=100,\n",
    "    early_stopping_patience=20,\n",
    "    epochs=400,\n",
    "    image_shape=300,\n",
    "    label_width=350,\n",
    "    learning_rate=0.0002,\n",
    "    lr_scheduler_factor=0.5,\n",
    "    mini_batch_size=5,\n",
    "    momentum=0.9,\n",
    "    nms_threshold=0.45,\n",
    "    num_classes=len(CLASS_NAMES),\n",
    "    num_training_samples=n_samples_training,\n",
    "    optimizer=\"sgd\",\n",
    "    overlap_threshold=0.5,\n",
    "    use_pretrained_model=1,\n",
    "    weight_decay=0.005,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the model\n",
    "\n",
    "The hyperparameters above represent our best up-front guess; and it's easy enough to call `estimator.fit()` to train a model as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({ \"train\": train_channel, \"validation\": validation_channel }, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Deploy the model\n",
    "\n",
    "Once a model is trained, SageMaker supports using it for either:\n",
    "\n",
    "* Deploying the model to an *endpoint* for real-time inference\n",
    "* Running a *batch transform* job on an input dataset\n",
    "\n",
    "In this example we'll deploy a real-time endpoint.\n",
    "\n",
    "Since our endpoints won't be handling any significant traffic volumes, we provision a single non-accelerated instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deploying standard model...\")\n",
    "predictor_std = estimator.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.m5.large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, if we ever lose notebook state e.g. due to a kernel restart or crash, we can `attach()` our estimator/tuner to a previous training/tuning job as follows: (No need to re-train - the results are all stored!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example to attach to a previous training run:\n",
    "#estimator.attach(\"bootsncats-ssd-hpo-191209-1637-003-bbdca4b2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run inference on test images\n",
    "\n",
    "Now we have one or more models deployed, we can send our same test images to them and see how they perform!\n",
    "\n",
    "The `visualize_detection()` function used here is provided in the `util` folder: it just uses matplotlib to plot the provided detection boxes over the image.\n",
    "\n",
    "Unlike Rekognition Custom Labels, the built-in Object Detection algorithm doesn't estimate an optimal confidence threshold for us. What number do you find gives best results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this if you want something different:\n",
    "predictor = predictor_std\n",
    "\n",
    "# This time confidence is 0-1, not 0-100:\n",
    "confidence_threshold= 0.3 # TODO: 0.2 is a good starting point, but explore options!\n",
    "\n",
    "for test_image in os.listdir(test_image_folder):\n",
    "    test_image_path = f\"{test_image_folder}/{test_image}\"\n",
    "    with open(test_image_path, \"rb\") as f:\n",
    "        payload = bytearray(f.read())\n",
    "\n",
    "    client = boto3.client(\"sagemaker-runtime\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=predictor.endpoint,\n",
    "        ContentType='application/x-image',\n",
    "        Body=payload\n",
    "    )\n",
    "\n",
    "    result = response['Body'].read()\n",
    "    result = json.loads(result)[\"prediction\"]\n",
    "    # print(result)\n",
    "    # result is a list of [class_ix, confidence, x1, y1, x2, y2] detections.\n",
    "    # (x/y locations are relative/normalized)\n",
    "    display(HTML(f\"<h4>{test_image}</h4>\"))\n",
    "    util.visualize_detection(\n",
    "        test_image_path,\n",
    "        result,\n",
    "        CLASS_NAMES,\n",
    "        thresh=confidence_threshold\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 (Optional): Automatic Hyperparamater Tuning (HPO)\n",
    "We can improve model performance and reduce some of the guesswork in setting these hyperparameters by letting the SageMaker `HyperParameterTuner` optimize them. SageMaker HPO uses a [Bayesian optimization](https://arxiv.org/abs/1807.02811) strategy (unless you [tell it otherwise](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html)) specifically formulated for this kind of expensive-to-evaluate optimization scenario: Much more cost efficient than naive options like grid search.\n",
    "\n",
    "Because HPO typically takes much longer than standard model fitting, `tuner.fit()` is an **asynchronous** method by default whereas `estimator.fit()` is **synchronous** (blocking).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "        \"learning_rate\": sagemaker.tuner.ContinuousParameter(0.0001, 0.1),\n",
    "        \"momentum\": sagemaker.tuner.ContinuousParameter(0.0, 0.99),\n",
    "        \"weight_decay\": sagemaker.tuner.ContinuousParameter(0.0, 0.99),\n",
    "        \"mini_batch_size\": sagemaker.tuner.IntegerParameter(1, n_samples_validation),\n",
    "        \"optimizer\": sagemaker.tuner.CategoricalParameter(['sgd', 'adam', 'rmsprop', 'adadelta'])\n",
    "}\n",
    "\n",
    "tuner = sagemaker.tuner.HyperparameterTuner(\n",
    "        estimator,\n",
    "        \"validation:mAP\",  # Name of the objective metric to optimize\n",
    "        objective_type=\"Maximize\",  # \"Mean Average Precision\" high = good\n",
    "        hyperparameter_ranges=hyperparameter_ranges,\n",
    "        base_tuning_job_name=\"bootsncats-ssd-hpo\",\n",
    "        # Defining the maximum number and parallelism of HPO training jobs:\n",
    "        # Note that accounts have protective limits on number of GPU instances by default.\n",
    "        # For Event Engine accounts, default max ml.p3.2xlarge = 2\n",
    "        # Set max_parallel_jobs = (limit / train_instance_count) - 1\n",
    "        # (minus one lets you run HPO and non-HPO in parallel)\n",
    "        max_jobs=20,# TODO: Ideally 20 or more\n",
    "        max_parallel_jobs=2# TODO: Maybe only 1 for Event Engine, more if possible\n",
    "    )\n",
    "    \n",
    "tuner.fit(\n",
    "        { \"train\": train_channel, \"validation\": validation_channel },\n",
    "        include_cls_metadata=False\n",
    ")\n",
    "\n",
    "# Examples to attach to a previous training run:\n",
    "#tuner.attach(\"bootsncats-ssd-hpo-191209-1637\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: While the model(s) are training\n",
    "\n",
    "Individual training jobs typically take around 10 minutes for this configuration and so the HPO job may take a couple of hours, depending on your configured `max_jobs`\n",
    "\n",
    "Take some time to familiarize yourself with the metrics reported in the *Training > Training jobs* and *Training > Hyperparameter tuning jobs* sections of the console: Both of which provide useful tracking for the inputs and parameters of training jobs as well as the result metrics. \n",
    "\n",
    "If you're running through other notebooks at the same time, now is a good time to go and check on those!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Deploy the HPO model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor_hpo = tuner.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.m5.large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Run inference on test images\n",
    "\n",
    "Now we have one or more models deployed, we can send our same test images to them and see how they perform!\n",
    "\n",
    "The `visualize_detection()` function used here is provided in the `util` folder: it just uses matplotlib to plot the provided detection boxes over the image.\n",
    "\n",
    "Unlike Rekognition Custom Labels, the built-in Object Detection algorithm doesn't estimate an optimal confidence threshold for us. What number do you find gives best results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this if you want something different:\n",
    "predictor = predictor_hpo\n",
    "\n",
    "# This time confidence is 0-1, not 0-100:\n",
    "confidence_threshold =.3 # TODO: 0.2 is a good starting point, but explore options!\n",
    "\n",
    "for test_image in os.listdir(test_image_folder):\n",
    "    test_image_path = f\"{test_image_folder}/{test_image}\"\n",
    "    with open(test_image_path, \"rb\") as f:\n",
    "        payload = bytearray(f.read())\n",
    "\n",
    "    client = boto3.client(\"sagemaker-runtime\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=predictor.endpoint,\n",
    "        ContentType='application/x-image',\n",
    "        Body=payload\n",
    "    )\n",
    "\n",
    "    result = response['Body'].read()\n",
    "    result = json.loads(result)[\"prediction\"]\n",
    "    # print(result)\n",
    "    # result is a list of [class_ix, confidence, x1, y1, x2, y2] detections.\n",
    "    # (x/y locations are relative/normalized)\n",
    "    display(HTML(f\"<h4>{test_image}</h4>\"))\n",
    "    util.visualize_detection(\n",
    "        test_image_path,\n",
    "        result,\n",
    "        CLASS_NAMES,\n",
    "        thresh=confidence_threshold\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Although training instances are ephemeral, the resources we allocated for real-time endpoints need to be cleaned up to avoid ongoing charges.\n",
    "\n",
    "The code below will delete the *most recently deployed* endpoint for the HPO and non-HPO configurations, but note that if you deployed either more than once, you might end up with extra endpoints.\n",
    "\n",
    "To be safe, it's best to still check through the SageMaker console for any left-over resources when cleaning up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete standard endpoint\n",
    "print(\"Deleting standard (non-HPO) predictor endpoint\")\n",
    "predictor_std.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete HPO endpoint\n",
    "print(\"Deleting HPO-optimized predictor endpoint\")\n",
    "predictor_hpo.delete_endpoint()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "In this notebook we used our SageMaker Ground Truth annotated dataset to train the built-in Object Detection algorithm for our use case.\n",
    "\n",
    "You probably found with this small dataset and starting hyperparameters that it was hard to get to the same level of performance as the automatic learning in Rekognition Custom Labels: but as can be seen we have much more control over the model (and our costs), which can be useful for situations where our team has knowledge of the problem and how to solve it well.\n",
    "\n",
    "The next step on the control/complexity continuum would be to use a custom algorithm in place of the built-in: A good fit for teams interested in exploring [other object detection procedures](https://arxiv.org/pdf/1908.03673.pdf) like YOLO, Fast(er)-RCNN, or any more recent advances, besides the SageMaker SSD implementation.\n",
    "\n",
    "Thanks for taking the time to explore this notebook and the others in the series: We'd love to hear your feedback!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
