{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boots 'n' Cats 2a: Modelling with Rekognition Custom Labels\n",
    "\n",
    "In this notebook we'll use [Amazon Rekognition](https://aws.amazon.com/rekognition/)'s new [custom labels](https://aws.amazon.com/rekognition/custom-labels-features/) functionality (announced at Re:Invent 2019) to train a boots 'n' cats detector with minimal data science knowledge required.\n",
    "\n",
    "You'll need to have gone through the first notebook in this series (*Intro and Data Preparation*) to complete this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the Rekognition Custom Labels project\n",
    "\n",
    "In the Rekognition Console (*Services > Amazon Rekognition* in the AWS console):\n",
    "\n",
    "* Select **Custom Labels** from the Rekognition sidebar menu (If you don't see this menu item, you might be in a region where Rekognition Custom Labels functionality is not yet available!)\n",
    "* Click the **Get started** button from the dashboard, or **Projects > Create project** from the expandable Rekognition Custom Labels sidebar menu\n",
    "* **Name** your project `bootsncats` and click create\n",
    "\n",
    "You should be forwarded to the project overview screen for your new, empty project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create training and validation datasets\n",
    "\n",
    "From the *Datasets* section, click the **Create dataset** button:\n",
    "\n",
    "* We'll name our training dataset `boots-and-cats`\n",
    "* Select the first type *\"Import images labeled by SageMaker Ground Truth\"*\n",
    "* For the **manifest location**, select the **training manifest S3 URI** as printed near the end of the intro and data preparation notebook\n",
    "\n",
    "<img src=\"BlogImages/CreateRekognitionDataset.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeat** this process for the validation data-set, this time naming the Rekognition dataset `boots-and-cats-validation`, and using the **validation manifest S3 URI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the Rekognition Custom Labels Model\n",
    "\n",
    "Now that both datasets are created, you can train a model.\n",
    "\n",
    "The project ARN should be pre-populated if you clicked *\"Train model\"* from the project details page; but if not you can find it from the output of Step 1 in this notebook.\n",
    "\n",
    "Select `boots-and-cats` for the training dataset, and `boots-and-cats-validation` for the test dataset, and click **Train** to get started.\n",
    "\n",
    "<img src=\"BlogImages/RekognitionTrainModel.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: While the model trains...\n",
    "\n",
    "Training the model can take several minutes, so why not use this time to start looking at the next notebook with a different modelling approach - SageMaker built-in algorithms?\n",
    "\n",
    "You can check up on the model training process in the AWS Console (you might need to refresh the page for latest updates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Review model metrics\n",
    "\n",
    "When the model training is complete, you should be able to click on the hyperlinked model name from the project details page, to access the model overview:\n",
    "\n",
    "<img src=\"BlogImages/RekognitionModelDetails.png\"/>\n",
    "\n",
    "A range of classification-oriented statistics ([precision, recall, and F1 score](https://en.wikipedia.org/wiki/Precision_and_recall)) are presented to help you evaluate the quality of the trained model versus the provided test/validation set and decide whether to deploy it: Including both summary/overall and per-class figures.\n",
    "\n",
    "Rekognition also calculates an **Assumed threshold** for each class: the \"optimal\" confidence score threshold detections must pass to be reported in the output.\n",
    "\n",
    "However, simply maximising the F1 score for this \"optimum\" isn't right for every use case: E.g. in fraud detection or medical disease detection the value of a false positive might be very different from a false negative. For this reason you can still override the threshold when making API requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Deploy the model\n",
    "\n",
    "[Pricing](https://aws.amazon.com/rekognition/pricing/) for Rekognition Custom Labels inference is by **capacity**: We deploy (\"start\") our model with a specified capacity which translates to transactions-per-second [as documented here](https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/rm-run-model.html).\n",
    "\n",
    "Note that the model may take a minute or two to start and stop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the **model ARN** to call the model in the next step. You can fetch it through the Console UI. (The Console UI currently only shows the model ARN *after* training is complete - by clicking on the hyperlinked model name in the project details page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6a: Dependencies and configuration\n",
    "\n",
    "As usual we'll start by loading libraries, defining configuration, and connecting to the AWS SDKs.\n",
    "\n",
    "For this particular notebook, we'll also **update** the core AWS libraries because Rekognition Custom Labels are so new!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade awscli boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# Built-Ins:\n",
    "import os\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import sagemaker\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Local Dependencies:\n",
    "%aimport util\n",
    "\n",
    "# Next we re-load configuration from the intro & data processing notebook:\n",
    "%store -r BUCKET_NAME\n",
    "assert BUCKET_NAME, \"BUCKET_NAME missing from IPython store\"\n",
    "%store -r CLASS_NAMES\n",
    "assert CLASS_NAMES, \"CLASS_NAMES missing from IPython store\"\n",
    "%store -r test_image_folder\n",
    "assert test_image_folder, \"test_image_folder missing from IPython store\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we connect to the AWS SDKs we'll use, and validate the choice of S3 bucket:\n",
    "\n",
    "**If you get a Rekognition permissions error:** The configured IAM Role for this SageMaker notebook instance doesn't have access to Rekognition.\n",
    "\n",
    "* Go to the notebook instance in the SageMaker console (*Notebook > Notebook Instances > Click on this notebook's name*)\n",
    "* Scroll down to *Permissions and encryption* and click on the hyperlinked \"IAM Role ARN\"\n",
    "* \"Attach Policies\" for *AmazonRekognitionFullAccess*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "rekognition = session.client(\"rekognition\")\n",
    "\n",
    "bucket_region = \\\n",
    "    session.client(\"s3\").head_bucket(Bucket=BUCKET_NAME)[\"ResponseMetadata\"][\"HTTPHeaders\"][\"x-amz-bucket-region\"]\n",
    "assert (\n",
    "    bucket_region == region\n",
    "), f\"Your S3 bucket {BUCKET_NAME} and this notebook need to be in the same region.\"\n",
    "\n",
    "if (region != \"us-east-1\"):\n",
    "    print(\"WARNING: Rekognition Custom Labels functionality is only available in us-east-1 at launch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6b: Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arn = #Copy the model ARN from the Console UI\n",
    "\n",
    "rekognition.start_project_version(\n",
    "    ProjectVersionArn= model_arn,\n",
    "    MinInferenceUnits=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Use the model\n",
    "\n",
    "When the model is `RUNNING` (this can take a couple of minutes), we can call it via API to perform inference on our test images as below.\n",
    "\n",
    "As discussed before, we can **override the *AssumedConfidence*** threshold selected by Rekognition - or even request **all** detections and filter the result with a `confidence_threshold` in post-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to \"None\" to use Rekognition's default, or else a float in 0-100\n",
    "# (Yes it's weird that Rekognition wants a pctage not a 0-1...)\n",
    "override_confidence =80 # TODO: Try None (no quotes), 80, and some other numbers\n",
    "\n",
    "for test_image in os.listdir(test_image_folder):\n",
    "    test_image_path = f\"{test_image_folder}/{test_image}\"\n",
    "    with open(test_image_path, \"rb\") as f:\n",
    "        payload = bytearray(f.read())\n",
    "        \n",
    "    response = rekognition.detect_custom_labels(\n",
    "        ProjectVersionArn=model_arn,\n",
    "        Image={ \"Bytes\": payload },\n",
    "        MinConfidence=override_confidence\n",
    "    )\n",
    "\n",
    "    result = list(map(\n",
    "        lambda l: [\n",
    "            CLASS_NAMES.index(l[\"Name\"]),  # Class ID\n",
    "            l[\"Confidence\"],  # Confidence score\n",
    "            l[\"Geometry\"][\"BoundingBox\"][\"Left\"],  # x1\n",
    "            l[\"Geometry\"][\"BoundingBox\"][\"Top\"],  # y1\n",
    "            l[\"Geometry\"][\"BoundingBox\"][\"Left\"]\n",
    "            + l[\"Geometry\"][\"BoundingBox\"][\"Width\"],  # x2\n",
    "            l[\"Geometry\"][\"BoundingBox\"][\"Top\"]\n",
    "            + l[\"Geometry\"][\"BoundingBox\"][\"Height\"],  # y2\n",
    "        ],\n",
    "        response[\"CustomLabels\"]\n",
    "    ))\n",
    "    # result is a list of [class_ix, confidence, x1, y1, x2, y2] detections.\n",
    "    display(HTML(f\"<h4>{test_image}</h4>\"))\n",
    "    util.visualize_detection(\n",
    "        test_image_path,\n",
    "        result,\n",
    "        CLASS_NAMES,\n",
    "        thresh=0 # No cutoff needed since Rekognition is already filtering for us\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "When we're done with our model, we should *stop* it to avoid any ongoing charges for inference capacity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rekognition.stop_project_version(\n",
    "    ProjectVersionArn=model_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "Hopefully you found it easy to create a Rekognition Custom Labels model deployment from our SageMaker Ground Truth labelled data: It's a great entry point for standard computer vision tasks without needing deep neural network skills, and you might find in our other notebooks that the automatic learning out-performs basic custom models unless they're very carefully optimized!\n",
    "\n",
    "Remember that the \"Manifest files\" we used as inputs for this model training were just JSONLines documents with a particular schema, and we actually already modified them in the data preparation notebook to standardize between batches: If you have image annotations in a different format, it's pretty easy to convert them for easy ingestion.\n",
    "\n",
    "Thanks for taking the time to explore this notebook and the others in the series: We'd love to hear your feedback!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
